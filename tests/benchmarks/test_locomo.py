"""
LoCoMo Benchmark Evaluation for AutoMem

Tests AutoMem's long-term conversational memory against the LoCoMo benchmark.
LoCoMo (ACL 2024) evaluates memory systems across 5 categories:
- Category 1: Single-hop recall (simple fact retrieval)
- Category 2: Temporal understanding (time-based queries)
- Category 3: Multi-hop reasoning (connecting multiple memories)
- Category 4: Open domain knowledge
- Category 5: Complex reasoning

Dataset: 10 conversations, 1,986 questions total
CORE (SOTA): 88.24% overall accuracy

References:
- Paper: https://github.com/snap-research/locomo/tree/main/static/paper/locomo.pdf
- Code: https://github.com/snap-research/locomo
- CORE blog: https://blog.heysol.ai/core-build-memory-knowledge-graph-for-individuals-and-achieved-sota-on-locomo-benchmark/
"""

import json
import os
import re
import sys
import time
from collections import defaultdict
from dataclasses import dataclass
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import requests
from dateutil import parser as date_parser
from openai import OpenAI

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent.parent))


@dataclass
class LoCoMoConfig:
    """Configuration for LoCoMo benchmark evaluation"""

    # AutoMem API settings
    base_url: str = os.getenv("AUTOMEM_TEST_BASE_URL", "http://localhost:8001")
    api_token: str = os.getenv("AUTOMEM_TEST_API_TOKEN", "test-token")

    # LoCoMo dataset paths
    data_file: str = str(Path(__file__).parent / "locomo" / "data" / "locomo10.json")

    # Evaluation settings
    recall_limit: int = 10  # Number of memories to retrieve per question
    importance_threshold: float = 0.5  # Minimum importance for stored memories

    # Tag configuration
    use_conversation_tags: bool = True  # Tag memories by conversation ID
    use_session_tags: bool = True  # Tag memories by session ID
    use_speaker_tags: bool = True  # Tag memories by speaker name

    # Scoring thresholds
    exact_match_threshold: float = 0.9  # For exact string matching
    fuzzy_match_threshold: float = 0.7  # For partial matches

    # Performance tuning
    batch_size: int = 50  # Memories to store before pausing
    pause_between_batches: float = 0.5  # Seconds to wait between batches


class LoCoMoEvaluator:
    """Evaluates AutoMem against the LoCoMo benchmark"""

    def __init__(self, config: LoCoMoConfig):
        self.config = config
        self.headers = {
            "Authorization": f"Bearer {config.api_token}",
            "Content-Type": "application/json",
        }
        self.memory_map = {}  # Maps dialog IDs to memory IDs
        self.results = defaultdict(list)  # Category -> [True/False scores]

        # Phase 2: Initialize OpenAI client for LLM-based answer extraction
        self.openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        # DISABLED: LLM extraction too slow for iteration; using word-overlap for now
        self.use_llm_extraction = False

        # Phase 2.5: Cache LLM responses to avoid redundant API calls
        self.llm_cache = {}  # (question, answer) -> (result, confidence, explanation)

        # Embedding-based answer checking (fast, handles semantic similarity)
        self.use_embedding_similarity = bool(os.getenv("OPENAI_API_KEY"))
        self.embedding_cache = {}  # text -> embedding vector

    def health_check(self) -> bool:
        """Verify AutoMem API is accessible"""
        try:
            response = requests.get(f"{self.config.base_url}/health", timeout=5)
            return response.status_code == 200
        except Exception as e:
            print(f"Health check failed: {e}")
            return False

    def _get_embedding(self, text: str) -> Optional[List[float]]:
        """Get embedding for text, with caching."""
        if not self.use_embedding_similarity:
            return None

        # Truncate and cache key
        text = text[:1000]  # Limit text length
        cache_key = text[:200]  # Cache by prefix

        if cache_key in self.embedding_cache:
            return self.embedding_cache[cache_key]

        try:
            response = self.openai_client.embeddings.create(
                model="text-embedding-3-small", input=text
            )
            embedding = response.data[0].embedding
            self.embedding_cache[cache_key] = embedding
            return embedding
        except Exception as e:
            return None

    def _cosine_similarity(self, a: List[float], b: List[float]) -> float:
        """Compute cosine similarity between two vectors."""
        import math

        dot = sum(x * y for x, y in zip(a, b))
        norm_a = math.sqrt(sum(x * x for x in a))
        norm_b = math.sqrt(sum(x * x for x in b))
        if norm_a == 0 or norm_b == 0:
            return 0.0
        return dot / (norm_a * norm_b)

    def cleanup_test_data(self, tag_prefix: str = "locomo-test"):
        """Remove all test memories from AutoMem"""
        print(f"\nüßπ Cleaning up test memories with tag: {tag_prefix}")
        try:
            # Use /recall endpoint which is more reliable for tag search
            # Loop until no more memories found (handles pagination)
            total_deleted = 0
            while True:
                response = requests.get(
                    f"{self.config.base_url}/recall",
                    headers=self.headers,
                    params={"tags": tag_prefix, "tag_match": "prefix", "limit": 100},
                )

                if response.status_code != 200:
                    print(f"‚ö†Ô∏è  Could not fetch test memories: {response.status_code}")
                    break

                results = response.json().get("results", [])
                if not results:
                    break

                # Delete each memory
                for r in results:
                    memory_id = r.get("id")
                    if memory_id:
                        requests.delete(
                            f"{self.config.base_url}/memory/{memory_id}", headers=self.headers
                        )
                        total_deleted += 1

                # If fewer than 100 returned, we're done
                if len(results) < 100:
                    break

            print(f"‚úÖ Cleaned up {total_deleted} test memories")
            return True

        except Exception as e:
            print(f"‚ö†Ô∏è  Cleanup error: {e}")
            return False

    def load_conversation_into_automem(
        self, conversation: Dict[str, Any], sample_id: str
    ) -> Dict[str, str]:
        """
        Load a LoCoMo conversation into AutoMem as individual memories.

        Returns mapping of dialog_id -> memory_id
        """
        memory_map = {}
        memory_count = 0

        print(f"\nüì• Loading conversation {sample_id} into AutoMem...")

        # Extract conversation metadata
        speaker_a = conversation["conversation"].get("speaker_a", "Speaker A")
        speaker_b = conversation["conversation"].get("speaker_b", "Speaker B")

        # Process each session
        session_keys = sorted(
            [
                k
                for k in conversation["conversation"].keys()
                if k.startswith("session_") and not k.endswith("_date_time")
            ]
        )

        for session_key in session_keys:
            session_num = session_key.split("_")[1]
            session_data = conversation["conversation"][session_key]
            session_datetime = conversation["conversation"].get(
                f"session_{session_num}_date_time", ""
            )

            # Store each dialog turn as a memory
            for turn in session_data:
                speaker = turn.get("speaker", "unknown")
                dia_id = turn.get("dia_id", f"unknown_{memory_count}")
                text = turn.get("text", "")
                img_url = turn.get("img_url")
                blip_caption = turn.get("blip_caption")

                if not text:
                    continue

                # Build memory content
                content = f"{speaker}: {text}"
                if blip_caption:
                    content += f" [Image: {blip_caption}]"

                # Build tags
                tags = [
                    f"locomo-test",
                    f"conversation:{sample_id}",
                    f"session:{session_num}",
                    f"speaker:{speaker.lower().replace(' ', '-')}",
                ]

                # Build metadata
                metadata = {
                    "source": "locomo_benchmark",
                    "conversation_id": sample_id,
                    "session_id": session_num,
                    "dialog_id": dia_id,
                    "speaker": speaker,
                    "session_datetime": session_datetime,
                }

                if img_url:
                    metadata["image_url"] = img_url
                if blip_caption:
                    metadata["image_caption"] = blip_caption

                # Store memory
                try:
                    response = requests.post(
                        f"{self.config.base_url}/memory",
                        headers=self.headers,
                        json={
                            "content": content,
                            "tags": tags,
                            "importance": self.config.importance_threshold,
                            "metadata": metadata,
                            "type": "Context",
                        },
                    )

                    if response.status_code in [200, 201]:  # Accept both OK and Created
                        result = response.json()
                        # API returns memory_id; be robust to historical 'id'
                        memory_id = result.get("memory_id") or result.get("id")
                        memory_map[dia_id] = memory_id
                        memory_count += 1

                        # Pause every N memories
                        if memory_count % self.config.batch_size == 0:
                            print(f"  Stored {memory_count} memories...")
                            time.sleep(self.config.pause_between_batches)
                    else:
                        print(
                            f"‚ö†Ô∏è  Failed to store memory for {dia_id}: {response.status_code} - {response.text[:100]}"
                        )

                except Exception as e:
                    print(f"‚ö†Ô∏è  Error storing memory for {dia_id}: {e}")

        print(f"‚úÖ Loaded {memory_count} memories from conversation {sample_id}")
        return memory_map

    def _extract_speaker_from_question(self, question: str) -> Optional[str]:
        """
        Extract person/speaker name from a question.

        For "Would Caroline pursue writing?" returns "Caroline"
        For "What did John say about?" returns "John"
        """
        import re

        # Common stopwords that look like names
        stopwords = {
            "What",
            "Would",
            "Could",
            "Does",
            "Did",
            "How",
            "Why",
            "When",
            "Where",
            "Which",
            "Who",
            "Whose",
            "Will",
            "Can",
            "Should",
            "Has",
            "Have",
            "Had",
            "Is",
            "Are",
            "Was",
            "Were",
            "Do",
            "Been",
            "Being",
            "The",
            "Answer",
            "Yes",
            "No",
            "Likely",
            "Based",
            "According",
            "Since",
            "Because",
        }

        words = question.split()
        for i, word in enumerate(words):
            # Clean punctuation
            clean_word = re.sub(r"[^\w]", "", word)

            if len(clean_word) < 2 or clean_word in stopwords:
                continue

            # Check for capitalized word (potential name)
            if len(clean_word) > 1 and clean_word[0].isupper() and clean_word[1:].islower():
                # Skip if first word (sentence start)
                if i == 0:
                    continue
                return clean_word

        # Check for possessives like "John's"
        possessives = re.findall(r"\b([A-Z][a-z]+)'s\b", question)
        if possessives:
            name = possessives[0]
            if name not in stopwords:
                return name

        return None

    def is_temporal_question(self, question: str) -> bool:
        """Detect if question is asking about time/dates"""
        temporal_keywords = [
            "when",
            "what time",
            "what date",
            "which year",
            "which month",
            "how long ago",
            "before",
            "after",
            "during",
            "since",
            "until",
            "first time",
            "last time",
            "recently",
            "previously",
        ]
        question_lower = question.lower()
        return any(keyword in question_lower for keyword in temporal_keywords)

    def extract_temporal_hints(self, question: str) -> List[str]:
        """Extract temporal hints from question to enhance query"""
        hints = []
        question_lower = question.lower()

        # Month names
        months = [
            "january",
            "february",
            "march",
            "april",
            "may",
            "june",
            "july",
            "august",
            "september",
            "october",
            "november",
            "december",
        ]
        for month in months:
            if month in question_lower:
                hints.append(month)

        # Year patterns (2020-2025)
        years = re.findall(r"\b(202[0-5])\b", question)
        hints.extend(years)

        return hints

    def extract_dates(self, text: str) -> List[datetime]:
        """
        Quick Win #1: Extract all date references from text using dateutil.

        This enables fuzzy date matching even when dates are formatted differently.
        Example: "January 15th 2024" matches "2024-01-15" or "Jan 15"
        """
        dates = []
        text = text.lower()

        # Split into words and try parsing phrases
        words = text.split()
        for i in range(len(words)):
            for length in range(1, min(6, len(words) - i + 1)):  # Try 1-5 word combinations
                phrase = " ".join(words[i : i + length])
                try:
                    # Use dateutil's flexible parser
                    date = date_parser.parse(phrase, fuzzy=True)
                    # Only accept reasonable dates (1900-2100)
                    if 1900 <= date.year <= 2100:
                        dates.append(date)
                except (ValueError, OverflowError):
                    pass

        return dates

    def match_dates_fuzzy(
        self, question: str, memory_content: str, tolerance_days: int = 1
    ) -> bool:
        """
        Quick Win #1: Match dates even if formatted differently.

        Returns True if any date in question matches any date in memory
        within tolerance_days (default: 1 day).
        """
        question_dates = self.extract_dates(question)
        memory_dates = self.extract_dates(memory_content)

        if not question_dates or not memory_dates:
            return False

        # Check for matches within tolerance
        for q_date in question_dates:
            for m_date in memory_dates:
                days_diff = abs((q_date - m_date).days)
                if days_diff <= tolerance_days:
                    return True

        return False

    def recall_for_question(
        self, question: str, sample_id: str, session_context: str = None, evidence_count: int = 1
    ) -> List[Dict[str, Any]]:
        """
        Query AutoMem to recall memories relevant to a question.

        Uses hybrid search: semantic + keyword + tags
        Enhanced with temporal detection and multi-hop support (Phase 1).
        """
        try:
            # Phase 1 Improvement: Detect question type and adjust parameters
            is_temporal = self.is_temporal_question(question)
            is_multihop = evidence_count > 1

            # Determine recall limit based on question complexity
            if is_multihop:
                limit = 100  # Increased for multi-hop to capture all evidence
            elif is_temporal:
                limit = 75  # More context for temporal questions
            else:
                limit = 50  # Standard limit

            # Build enhanced query
            query = question

            # Phase 1 Improvement: Add temporal context to query
            if is_temporal:
                temporal_hints = self.extract_temporal_hints(question)
                if temporal_hints:
                    query = f"{question} {' '.join(temporal_hints)}"

            # Build query parameters
            params = {
                "query": query,
                "limit": limit,
                "tags": f"conversation:{sample_id}",
                "tag_match": "exact",
            }

            # Use auto_decompose and entity expansion for multi-hop questions
            if is_multihop:
                params["auto_decompose"] = "true"
                params["expand_entities"] = "true"  # Enable entity-to-entity expansion

            response = requests.get(
                f"{self.config.base_url}/recall", headers=self.headers, params=params
            )

            memories = []
            if response.status_code == 200:
                result = response.json()
                # AutoMem returns "results" with nested "memory" objects
                results = result.get("results", [])
                # Extract the memory objects from each result
                memories = [r.get("memory", {}) for r in results if "memory" in r]

            # Multi-hop enhancement: Also fetch memories by speaker tag
            # This catches memories that semantic search misses
            if is_multihop:
                # Extract person names from question
                speaker_name = self._extract_speaker_from_question(question)
                if speaker_name:
                    speaker_response = requests.get(
                        f"{self.config.base_url}/recall",
                        headers=self.headers,
                        params=[
                            ("tags", f"speaker:{speaker_name.lower()}"),
                            ("tags", f"conversation:{sample_id}"),
                            ("tag_mode", "all"),
                            ("tag_match", "exact"),
                            ("limit", "50"),
                        ],
                    )
                    if speaker_response.status_code == 200:
                        speaker_results = speaker_response.json().get("results", [])
                        speaker_memories = [
                            r.get("memory", {}) for r in speaker_results if "memory" in r
                        ]
                        # Add unique speaker memories not already in results
                        existing_ids = {m.get("id") for m in memories if m.get("id")}
                        for sm in speaker_memories:
                            if sm.get("id") not in existing_ids:
                                memories.append(sm)

            return memories

        except Exception as e:
            print(f"‚ö†Ô∏è  Recall error: {e}")
            return []

    def normalize_answer(self, text: str) -> str:
        """Normalize text for comparison with basic stemming"""
        # Convert to lowercase
        text = text.lower()
        # Remove punctuation
        text = re.sub(r"[^\w\s]", "", text)
        # Basic stemming for common suffixes
        words = text.split()
        stemmed = []
        for word in words:
            # Remove common verb/noun suffixes for matching
            if word.endswith("ing"):
                word = word[:-3]  # counseling -> counsel
            elif word.endswith("ed"):
                word = word[:-2]
            elif word.endswith("er") and len(word) > 4:
                word = word[:-2]  # counselor -> counsel
            elif word.endswith("or") and len(word) > 4:
                word = word[:-2]  # counselor -> counsel
            elif word.endswith("tion"):
                word = word[:-4]
            elif word.endswith("ment"):
                word = word[:-4]
            elif word.endswith("ness"):
                word = word[:-4]
            elif word.endswith("ful"):
                word = word[:-3]
            elif word.endswith("ly"):
                word = word[:-2]
            elif word.endswith("s") and len(word) > 3:
                word = word[:-1]  # plural
            stemmed.append(word)
        # Normalize whitespace
        return " ".join(stemmed)

    def fetch_evidence_memories(
        self, evidence_dialog_ids: List[str], sample_id: str
    ) -> List[Dict[str, Any]]:
        """
        Phase 2.5: Fetch specific evidence memories by dialog ID.

        This is more precise than semantic search - we know exactly which
        memories contain the answer based on the benchmark's evidence field.
        """
        evidence_memories = []

        try:
            # Get all memories for this conversation
            response = requests.get(
                f"{self.config.base_url}/recall",
                headers=self.headers,
                params={
                    "query": "",  # Empty query to get all
                    "limit": 1000,  # High limit to get all conversation memories
                    "tags": f"conversation:{sample_id}",
                    "tag_match": "exact",
                },
                timeout=10,
            )

            if response.status_code == 200:
                result = response.json()
                results = result.get("results", [])
                all_memories = [r.get("memory", {}) for r in results if "memory" in r]

                # Filter to just the evidence dialogs
                for memory in all_memories:
                    metadata = memory.get("metadata", {})
                    dialog_id = metadata.get("dialog_id", "")
                    if dialog_id in evidence_dialog_ids:
                        evidence_memories.append(memory)

        except Exception as e:
            print(f"‚ö†Ô∏è  Evidence fetch error: {e}")

        return evidence_memories

    def multi_hop_recall_with_graph(
        self, question: str, sample_id: str, initial_limit: int = 20, max_connected: int = 50
    ) -> List[Dict[str, Any]]:
        """
        Quick Win #2: Use graph traversal to find connected memories for multi-hop questions.

        Strategy:
        1. Get initial memories via semantic search
        2. For top N memories, traverse graph relationships
        3. Combine initial + connected memories
        4. Deduplicate and return

        This leverages AutoMem's relationship graph to find evidence that's
        connected via RELATES_TO, LEADS_TO, PART_OF edges.
        """
        try:
            # Step 1: Get initial memories
            initial_memories = self.recall_for_question(
                question, sample_id, evidence_count=2  # Trigger multi-hop handling
            )

            # Step 2: Extract memory IDs from top results
            memory_ids = []
            for mem in initial_memories[:initial_limit]:
                mem_id = mem.get("id")
                if mem_id:
                    memory_ids.append(mem_id)

            if not memory_ids:
                return initial_memories

            # Step 3: Traverse graph to find connected memories
            connected_memories = []

            for mem_id in memory_ids:
                try:
                    # Query AutoMem's graph traversal endpoint
                    response = requests.get(
                        f"{self.config.base_url}/memories/{mem_id}/related",
                        headers=self.headers,
                        params={
                            # Include enrichment + temporal + creative relations
                            "relationship_types": "RELATES_TO,LEADS_TO,PART_OF,DERIVED_FROM,SIMILAR_TO,PRECEDED_BY,EXPLAINS,SHARES_THEME,PARALLEL_CONTEXT",
                            "max_depth": 2,  # Two hops tends to be enough for LoCoMo
                            "limit": 8,  # Slightly higher cap per seed
                        },
                        timeout=5,
                    )

                    if response.status_code == 200:
                        result = response.json()
                        related = result.get("related_memories", [])
                        connected_memories.extend(related)

                except Exception as e:
                    # Silently continue if endpoint doesn't exist yet
                    pass

            # Step 4: Combine and deduplicate
            all_memories = initial_memories + connected_memories
            unique_map = {}
            for mem in all_memories:
                mem_id = mem.get("id")
                if mem_id and mem_id not in unique_map:
                    unique_map[mem_id] = mem

            result = list(unique_map.values())

            # Limit total results
            return result[:max_connected]

        except Exception as e:
            print(f"‚ö†Ô∏è  Graph traversal error: {e}")
            # Fallback to regular recall
            return self.recall_for_question(question, sample_id, evidence_count=2)

    def llm_extract_answer(
        self,
        question: str,
        expected_answer: Any,
        recalled_memories: List[Dict[str, Any]],
        is_multi_hop: bool = False,
    ) -> Tuple[bool, float, str]:
        """
        Phase 2: Use GPT-4o-mini to determine if recalled memories contain the answer.

        This is more sophisticated than word matching - the LLM can understand
        paraphrasing, synonyms, and contextual equivalence.

        Phase 2.5: Includes caching to avoid redundant API calls.
        Quick Win #3: Chain-of-thought reasoning for multi-hop questions.
        """
        if not self.use_llm_extraction or not recalled_memories:
            return None, 0.0, "LLM extraction disabled or no memories"

        # Check cache first
        # Fix: Handle list answers by converting to JSON string
        answer_str = (
            json.dumps(expected_answer, sort_keys=True)
            if isinstance(expected_answer, (list, dict))
            else str(expected_answer)
        )
        # Ensure is_multi_hop is bool
        is_multi_hop_bool = bool(is_multi_hop)
        cache_key = (question[:200], answer_str[:100], is_multi_hop_bool)
        if cache_key in self.llm_cache:
            return self.llm_cache[cache_key]

        try:
            # Build context from top recalled memories (limit to top 10 for token efficiency)
            memory_contexts = []
            for i, mem in enumerate(recalled_memories[:10]):
                content = mem.get("content", "")
                metadata = mem.get("metadata", {})
                dialog_id = metadata.get("dialog_id", f"mem-{i}")
                session = metadata.get("session_datetime", "")

                context = f"[{dialog_id}] {content}"
                if session:
                    context += f" (Session: {session})"
                memory_contexts.append(context)

            memories_text = "\n\n".join(memory_contexts)

            # Quick Win #3: Use chain-of-thought for multi-hop questions
            if is_multi_hop:
                # Chain-of-thought prompt for multi-hop reasoning
                prompt = f"""You are evaluating whether a conversation history contains the answer to a MULTI-HOP question.

Question: {question}
Expected Answer: {expected_answer}

Conversation History:
{memories_text}

This question requires CONNECTING MULTIPLE pieces of information. Think step-by-step:

1. What information pieces are needed to answer this question?
2. Which memories contain each piece?
3. How do these pieces connect to form the complete answer?
4. Does the connected information match the expected answer?

Respond in JSON format:
{{
    "reasoning_chain": ["piece 1 found in...", "piece 2 found in...", "connection: ..."],
    "evidence_memories": ["memory IDs used"],
    "contains_answer": true or false,
    "confidence": 0.0 to 1.0,
    "reasoning": "summary of how pieces connect"
}}"""
            else:
                # Standard prompt for simple questions
                prompt = f"""You are evaluating whether a conversation history contains the answer to a question.

Question: {question}
Expected Answer: {expected_answer}

Conversation History:
{memories_text}

Task: Determine if the conversation history contains information that answers the question with the expected answer (or something semantically equivalent).

Consider:
- Paraphrasing and synonyms
- Context and implied meaning
- Temporal information in session metadata
- The answer may be stated differently but mean the same thing

Respond in JSON format:
{{
    "contains_answer": true or false,
    "confidence": 0.0 to 1.0,
    "reasoning": "brief explanation of why the answer is/isn't present"
}}"""

            # Call GPT-4o-mini
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system",
                        "content": "You are a precise evaluator of question-answering systems.",
                    },
                    {"role": "user", "content": prompt},
                ],
                temperature=0.0,
                max_tokens=200,
                response_format={"type": "json_object"},
            )

            # Parse response
            result = json.loads(response.choices[0].message.content)
            contains_answer = result.get("contains_answer", False)
            confidence = float(result.get("confidence", 0.0))
            reasoning = result.get("reasoning", "")

            # Cache the result
            llm_result = (contains_answer, confidence, f"LLM: {reasoning}")
            self.llm_cache[cache_key] = llm_result

            return llm_result

        except Exception as e:
            print(f"‚ö†Ô∏è  LLM extraction error: {e}")
            error_result = (None, 0.0, f"LLM error: {str(e)}")
            self.llm_cache[cache_key] = error_result  # Cache errors too
            return error_result

    def check_answer_in_memories(
        self,
        question: str,
        expected_answer: Any,
        recalled_memories: List[Dict[str, Any]],
        evidence_dialog_ids: List[str] = None,
        sample_id: str = None,
    ) -> Tuple[bool, float, str]:
        """
        Check if the expected answer can be found in recalled memories.

        Phase 2.5: Fetches evidence memories directly if IDs are provided.
        Phase 2: Tries LLM-based extraction first, falls back to word matching.
        Phase 1: Enhanced with temporal metadata matching.

        Returns:
            (is_correct, confidence_score, explanation)
        """
        if not recalled_memories:
            return False, 0.0, "No memories recalled"

        # Quick Win #2: Detect multi-hop questions
        is_multi_hop = evidence_dialog_ids and len(evidence_dialog_ids) > 1

        # Phase 2.5: If we have evidence IDs, fetch them directly and combine with recalled
        if evidence_dialog_ids and sample_id:
            evidence_memories = self.fetch_evidence_memories(evidence_dialog_ids, sample_id)
            if evidence_memories:
                # Combine evidence with recalled (evidence first for priority)
                combined_memories = evidence_memories + [
                    m for m in recalled_memories if m not in evidence_memories
                ]
                recalled_memories = combined_memories[:80]  # Slightly higher cap

                # Quick Win: Multi-hop joining ‚Äî evaluate on concatenated evidence
                if len(evidence_dialog_ids) > 1:
                    # Build a single searchable text by concatenating evidence contents and session times
                    joined_text_parts = []
                    for mem in evidence_memories:
                        content = mem.get("content", "")
                        metadata = mem.get("metadata", {})
                        session_dt = metadata.get("session_datetime", "")
                        joined_text_parts.append(str(content))
                        if session_dt:
                            joined_text_parts.append(str(session_dt))
                    joined_text = " \n ".join(joined_text_parts).lower()
                    joined_norm = self.normalize_answer(joined_text)

                    # For temporal questions, try fuzzy date matching across the joined evidence
                    if self.is_temporal_question(question) and self.match_dates_fuzzy(
                        question, joined_text
                    ):
                        return True, 0.95, "Multi-hop: date match across joined evidence"

                    expected_str = str(expected_answer).lower()
                    expected_norm = self.normalize_answer(expected_str)
                    exp_words = set(expected_norm.split())
                    if exp_words:
                        overlap = exp_words.intersection(set(joined_norm.split()))
                        conf = len(overlap) / max(len(exp_words), 1)
                        # Lower threshold than single-memory since multiple pieces are needed
                        if conf >= 0.35:
                            return (
                                True,
                                conf,
                                f"Multi-hop: found answer across joined evidence (confidence: {conf:.2f})",
                            )

        # Phase 2: Try LLM-based answer extraction first
        # Quick Win #3: Pass is_multi_hop flag for chain-of-thought reasoning
        if self.use_llm_extraction:
            llm_result, llm_confidence, llm_explanation = self.llm_extract_answer(
                question, expected_answer, recalled_memories, is_multi_hop=is_multi_hop
            )

            # If LLM gave a definitive answer, use it
            if llm_result is not None and llm_confidence >= 0.6:
                return llm_result, llm_confidence, llm_explanation

        # Fallback to word-based matching
        # Normalize expected answer
        expected_str = str(expected_answer).lower()
        expected_normalized = self.normalize_answer(expected_str)

        # Phase 1 Improvement: Check if this is a temporal question
        is_temporal = self.is_temporal_question(question)

        # Strategy 1: If we have evidence dialog IDs, check only those memories
        if evidence_dialog_ids:
            for memory in recalled_memories:
                metadata = memory.get("metadata", {})
                dialog_id = metadata.get("dialog_id", "")

                # Check if this memory is one of the evidence dialogs
                if dialog_id in evidence_dialog_ids:
                    content = memory.get("content", "").lower()
                    content_normalized = self.normalize_answer(content)

                    # Phase 1 Improvement: For temporal questions, also check session_datetime
                    if is_temporal:
                        session_datetime = metadata.get("session_datetime", "").lower()
                        # Combine content and datetime for temporal matching
                        searchable_text = f"{content_normalized} {session_datetime}"

                        # Quick Win #1: Fuzzy date matching for temporal questions
                        if self.match_dates_fuzzy(question, content + " " + session_datetime):
                            return True, 0.95, f"Date match in evidence dialog {dialog_id}"
                    else:
                        searchable_text = content_normalized

                    # Much more lenient matching for evidence dialogs
                    # Just check if key words from answer appear
                    expected_words = set(expected_normalized.split())
                    searchable_words = set(searchable_text.split())
                    overlap = expected_words.intersection(searchable_words)

                    if len(expected_words) == 0:
                        confidence = 0.0
                    else:
                        confidence = len(overlap) / len(expected_words)

                    # If at least 30% of answer words appear in evidence dialog, count as correct
                    if confidence >= 0.3:
                        return (
                            True,
                            confidence,
                            f"Found in evidence dialog {dialog_id} (confidence: {confidence:.2f})",
                        )

        # Strategy 2: Semantic search through all recalled memories
        max_confidence = 0.0
        max_embed_confidence = 0.0
        found_in_memory = None
        answer_embedding = None  # Lazy-load only if word overlap fails

        for memory in recalled_memories:
            content = memory.get("content", "").lower()
            content_normalized = self.normalize_answer(content)

            # Exact substring match
            if expected_normalized in content_normalized:
                confidence = 1.0
                found_in_memory = memory.get("id")
                return True, confidence, f"Exact match in memory (confidence: {confidence:.2f})"

            # Fuzzy word overlap
            expected_words = set(expected_normalized.split())
            if len(expected_words) == 0:
                continue

            content_words = set(content_normalized.split())
            overlap = expected_words.intersection(content_words)

            if overlap:
                confidence = len(overlap) / len(expected_words)
                if confidence > max_confidence:
                    max_confidence = confidence
                    found_in_memory = memory.get("id")

        # If word overlap is sufficient, skip expensive embedding computation
        if max_confidence >= 0.5:
            return True, max_confidence, f"Found answer (confidence: {max_confidence:.2f})"

        # For multi-hop with insufficient word overlap, try embedding similarity
        if is_multi_hop and self.use_embedding_similarity and max_confidence < 0.5:
            # Lazy-load answer embedding only when needed
            if answer_embedding is None:
                qa_text = f"Question: {question} Answer: {str(expected_answer)}"
                answer_embedding = self._get_embedding(qa_text)

            if answer_embedding:
                # Only check top 10 memories for embeddings (speed optimization)
                for memory in recalled_memories[:10]:
                    content_embedding = self._get_embedding(memory.get("content", ""))
                    if content_embedding:
                        embed_sim = self._cosine_similarity(answer_embedding, content_embedding)
                        if embed_sim > max_embed_confidence:
                            max_embed_confidence = embed_sim

        # For multi-hop, use embedding similarity if it exceeds threshold
        # Embedding similarity: 0.50+ indicates semantic relevance for Q&A pairs
        if is_multi_hop and max_embed_confidence > 0.50:
            return (
                True,
                max_embed_confidence,
                f"Embedding match (similarity: {max_embed_confidence:.2f})",
            )

        # Determine if correct based on confidence
        is_correct = max_confidence >= 0.5

        if is_correct:
            explanation = f"Found answer (confidence: {max_confidence:.2f})"
        elif max_embed_confidence > 0:
            explanation = (
                f"No good match (word: {max_confidence:.2f}, embed: {max_embed_confidence:.2f})"
            )
        else:
            explanation = f"No good match (max: {max_confidence:.2f})"

        return is_correct, max_confidence, explanation

    def evaluate_conversation(self, conversation: Dict[str, Any], sample_id: str) -> Dict[str, Any]:
        """
        Evaluate a single LoCoMo conversation.

        Process:
        1. Load conversation into AutoMem
        2. For each question, recall relevant memories
        3. Check if answer is in recalled memories
        4. Calculate accuracy per category
        """
        print(f"\n{'='*60}")
        print(f"Evaluating Conversation: {sample_id}")
        print(f"{'='*60}")

        # Step 1: Load conversation
        memory_map = self.load_conversation_into_automem(conversation, sample_id)

        # Wait for enrichment to process (optional)
        print("\n‚è≥ Waiting for enrichment pipeline...")
        time.sleep(2)

        # Step 2: Evaluate each question
        qa_results = []
        questions = conversation.get("qa", [])

        print(f"\n‚ùì Evaluating {len(questions)} questions...")

        for i, qa in enumerate(questions):
            question = qa.get("question", "")
            answer = qa.get("answer", "")
            category = qa.get("category", 0)
            evidence = qa.get("evidence", [])

            # Recall memories for this question
            # Use graph expansion for multi-hop questions (evidence > 1)
            if evidence and len(evidence) > 1:
                recalled_memories = self.multi_hop_recall_with_graph(
                    question,
                    sample_id,
                    initial_limit=20,
                    max_connected=60,
                )
            else:
                recalled_memories = self.recall_for_question(
                    question,
                    sample_id,
                    evidence_count=len(evidence),
                )

            # Check if answer is in recalled memories
            # Phase 2.5: Pass sample_id to enable evidence fetching
            is_correct, confidence, explanation = self.check_answer_in_memories(
                question, answer, recalled_memories, evidence, sample_id
            )

            # Record result
            qa_result = {
                "question": question,
                "expected_answer": answer,
                "category": category,
                "is_correct": is_correct,
                "confidence": confidence,
                "recalled_count": len(recalled_memories),
                "explanation": explanation,
            }
            qa_results.append(qa_result)

            # Track results by category
            self.results[category].append(is_correct)

            # Progress indicator
            if (i + 1) % 10 == 0:
                print(f"  Processed {i+1}/{len(questions)} questions...")

        # Calculate conversation-level statistics
        correct_count = sum(1 for r in qa_results if r["is_correct"])
        total_count = len(qa_results)
        accuracy = correct_count / total_count if total_count > 0 else 0.0

        print(f"\nüìä Conversation Results:")
        print(f"  Accuracy: {accuracy:.2%} ({correct_count}/{total_count})")

        return {
            "sample_id": sample_id,
            "total_questions": total_count,
            "correct": correct_count,
            "accuracy": accuracy,
            "qa_results": qa_results,
            "memory_count": len(memory_map),
        }

    def run_benchmark(self, cleanup_after: bool = True) -> Dict[str, Any]:
        """
        Run the complete LoCoMo benchmark evaluation.

        Returns comprehensive results including per-category accuracy.
        """
        print("\n" + "=" * 60)
        print("üß† AutoMem LoCoMo Benchmark Evaluation")
        print("=" * 60)

        # Health check
        print("\nüè• Checking AutoMem health...")
        if not self.health_check():
            raise ConnectionError("AutoMem API is not accessible")
        print("‚úÖ AutoMem is healthy")

        # Cleanup existing test data
        self.cleanup_test_data()

        # Load dataset
        print(f"\nüìÇ Loading LoCoMo dataset from: {self.config.data_file}")
        with open(self.config.data_file, "r") as f:
            conversations = json.load(f)

        print(f"‚úÖ Loaded {len(conversations)} conversations")

        # Evaluate each conversation
        conversation_results = []
        start_time = time.time()

        for i, conversation in enumerate(conversations):
            sample_id = conversation.get("sample_id", f"sample_{i}")

            try:
                result = self.evaluate_conversation(conversation, sample_id)
                conversation_results.append(result)
            except Exception as e:
                print(f"‚ùå Error evaluating conversation {sample_id}: {e}")
                continue

        elapsed_time = time.time() - start_time

        # Calculate overall statistics
        print("\n" + "=" * 60)
        print("üìä FINAL RESULTS")
        print("=" * 60)

        # Overall accuracy
        total_questions = sum(r["total_questions"] for r in conversation_results)
        total_correct = sum(r["correct"] for r in conversation_results)
        overall_accuracy = total_correct / total_questions if total_questions > 0 else 0.0

        print(f"\nüéØ Overall Accuracy: {overall_accuracy:.2%} ({total_correct}/{total_questions})")
        print(f"‚è±Ô∏è  Total Time: {elapsed_time:.1f}s")
        print(f"üíæ Total Memories Stored: {sum(r['memory_count'] for r in conversation_results)}")

        # Category breakdown
        print("\nüìà Category Breakdown:")
        category_names = {
            1: "Single-hop Recall",
            2: "Temporal Understanding",
            3: "Multi-hop Reasoning",
            4: "Open Domain",
            5: "Complex Reasoning",
        }

        category_results = {}
        for category, scores in sorted(self.results.items()):
            correct = sum(scores)
            total = len(scores)
            accuracy = correct / total if total > 0 else 0.0
            category_results[category] = {
                "name": category_names.get(category, f"Category {category}"),
                "accuracy": accuracy,
                "correct": correct,
                "total": total,
            }
            print(
                f"  {category_names.get(category, f'Category {category}'):25s}: {accuracy:6.2%} ({correct:3d}/{total:3d})"
            )

        # Comparison with CORE
        core_sota = 0.8824
        improvement = overall_accuracy - core_sota
        print(f"\nüèÜ Comparison with CORE (SOTA):")
        print(f"  CORE: {core_sota:.2%}")
        print(f"  AutoMem: {overall_accuracy:.2%}")
        if improvement > 0:
            print(f"  üéâ AutoMem BEATS CORE by {improvement:.2%}!")
        elif improvement < 0:
            print(f"  üìâ AutoMem is {abs(improvement):.2%} behind CORE")
        else:
            print(f"  ü§ù AutoMem matches CORE")

        # Cleanup
        if cleanup_after:
            self.cleanup_test_data()

        # Return comprehensive results
        return {
            "overall": {
                "accuracy": overall_accuracy,
                "correct": total_correct,
                "total": total_questions,
                "elapsed_time": elapsed_time,
            },
            "categories": category_results,
            "conversations": conversation_results,
            "comparison": {
                "core_sota": core_sota,
                "automem": overall_accuracy,
                "improvement": improvement,
            },
        }


def main():
    """Run LoCoMo benchmark evaluation"""
    import argparse

    parser = argparse.ArgumentParser(description="Evaluate AutoMem on LoCoMo benchmark")
    parser.add_argument(
        "--base-url",
        default=os.getenv("AUTOMEM_TEST_BASE_URL", "http://localhost:8001"),
        help="AutoMem API base URL",
    )
    parser.add_argument(
        "--api-token",
        default=os.getenv("AUTOMEM_TEST_API_TOKEN", "test-token"),
        help="AutoMem API token",
    )
    parser.add_argument("--data-file", default=None, help="Path to locomo10.json")
    parser.add_argument(
        "--recall-limit", type=int, default=10, help="Number of memories to recall per question"
    )
    parser.add_argument(
        "--no-cleanup", action="store_true", help="Don't cleanup test data after evaluation"
    )
    parser.add_argument("--output", default=None, help="Save results to JSON file")
    parser.add_argument(
        "--test-one", action="store_true", help="Test with just one conversation for debugging"
    )

    args = parser.parse_args()

    # Build config
    config = LoCoMoConfig(
        base_url=args.base_url, api_token=args.api_token, recall_limit=args.recall_limit
    )

    if args.data_file:
        config.data_file = args.data_file

    # Run evaluation
    evaluator = LoCoMoEvaluator(config)
    results = evaluator.run_benchmark(cleanup_after=not args.no_cleanup)

    # Save results
    if args.output:
        with open(args.output, "w") as f:
            json.dump(results, f, indent=2)
        print(f"\nüíæ Results saved to: {args.output}")

    # Return exit code based on success
    return 0 if results["overall"]["accuracy"] > 0 else 1


if __name__ == "__main__":
    sys.exit(main())
